# Comprehensive experiment configurations for systematic parameter testing

experiments:
  # === BASELINE EXPERIMENTS ===
  baseline:
    name: "baseline"
    description: "Conservative baseline with strict tracking"
    learning_rate: 1e-4
    clip_range: 0.1
    ent_coef: 0.001
    n_epochs: 5
    batch_size: 128
    reward_weights:
      tracking: 15.0
      height: 1.0
      velocity: 2.0
    action_smoothing: 0.1

  # === LEARNING RATE VARIATIONS ===
  ultra_low_lr:
    name: "ultra_low_lr"
    description: "Ultra-low learning rate for very stable learning"
    learning_rate: 5e-5
    clip_range: 0.08
    ent_coef: 0.0005
    n_epochs: 3
    batch_size: 256
    reward_weights:
      tracking: 12.0
      height: 1.5
      velocity: 2.0
    action_smoothing: 0.05

  low_lr:
    name: "low_lr"
    description: "Low learning rate with conservative updates"
    learning_rate: 8e-5
    clip_range: 0.1
    ent_coef: 0.002
    n_epochs: 4
    batch_size: 192
    reward_weights:
      tracking: 10.0
      height: 2.0
      velocity: 2.5
    action_smoothing: 0.08

  medium_lr:
    name: "medium_lr"
    description: "Medium learning rate balanced approach"
    learning_rate: 2e-4
    clip_range: 0.15
    ent_coef: 0.005
    n_epochs: 8
    batch_size: 64
    reward_weights:
      tracking: 8.0
      height: 2.0
      velocity: 3.0
    action_smoothing: 0.1

  high_lr:
    name: "high_lr"
    description: "High learning rate for fast exploration"
    learning_rate: 5e-4
    clip_range: 0.2
    ent_coef: 0.01
    n_epochs: 10
    batch_size: 32
    reward_weights:
      tracking: 6.0
      height: 2.5
      velocity: 3.5
    action_smoothing: 0.15

  very_high_lr:
    name: "very_high_lr"
    description: "Very high learning rate for aggressive learning"
    learning_rate: 1e-3
    clip_range: 0.25
    ent_coef: 0.02
    n_epochs: 12
    batch_size: 32
    reward_weights:
      tracking: 5.0
      height: 3.0
      velocity: 4.0
    action_smoothing: 0.2

  # === ENTROPY VARIATIONS (Exploration vs Exploitation) ===
  no_entropy:
    name: "no_entropy"
    description: "Pure exploitation, no exploration"
    learning_rate: 1e-4
    clip_range: 0.1
    ent_coef: 0.0
    n_epochs: 5
    batch_size: 128
    reward_weights:
      tracking: 15.0
      height: 1.0
      velocity: 2.0
    action_smoothing: 0.05

  low_entropy:
    name: "low_entropy"
    description: "Low entropy for focused learning"
    learning_rate: 1.5e-4
    clip_range: 0.12
    ent_coef: 0.001
    n_epochs: 6
    batch_size: 96
    reward_weights:
      tracking: 12.0
      height: 1.5
      velocity: 2.5
    action_smoothing: 0.08

  high_entropy:
    name: "high_entropy"
    description: "High entropy for 80D muscle exploration"
    learning_rate: 3e-4
    clip_range: 0.18
    ent_coef: 0.015
    n_epochs: 8
    batch_size: 64
    reward_weights:
      tracking: 8.0
      height: 2.0
      velocity: 3.0
    action_smoothing: 0.12

  very_high_entropy:
    name: "very_high_entropy"
    description: "Very high entropy for maximum exploration"
    learning_rate: 4e-4
    clip_range: 0.22
    ent_coef: 0.025
    n_epochs: 10
    batch_size: 48
    reward_weights:
      tracking: 6.0
      height: 2.5
      velocity: 3.5
    action_smoothing: 0.15

  # === NETWORK SIZE VARIATIONS ===
  tiny_network:
    name: "tiny_network"
    description: "Tiny networks for fast, simple learning"
    learning_rate: 3e-4
    clip_range: 0.15
    ent_coef: 0.008
    n_epochs: 8
    batch_size: 64
    reward_weights:
      tracking: 10.0
      height: 2.0
      velocity: 3.0
    action_smoothing: 0.1
    network_size: [128, 128]

  small_network:
    name: "small_network"
    description: "Small networks for stable learning"
    learning_rate: 2.5e-4
    clip_range: 0.15
    ent_coef: 0.006
    n_epochs: 8
    batch_size: 64
    reward_weights:
      tracking: 10.0
      height: 2.0
      velocity: 3.0
    action_smoothing: 0.1
    network_size: [256, 256, 128]

  large_network:
    name: "large_network"
    description: "Large networks for complex muscle patterns"
    learning_rate: 1.5e-4
    clip_range: 0.12
    ent_coef: 0.004
    n_epochs: 6
    batch_size: 96
    reward_weights:
      tracking: 10.0
      height: 2.0
      velocity: 3.0
    action_smoothing: 0.1
    network_size: [512, 512, 512, 256]

  huge_network:
    name: "huge_network"
    description: "Huge networks for maximum capacity"
    learning_rate: 1e-4
    clip_range: 0.1
    ent_coef: 0.003
    n_epochs: 5
    batch_size: 128
    reward_weights:
      tracking: 10.0
      height: 2.0
      velocity: 3.0
    action_smoothing: 0.1
    network_size: [1024, 1024, 512, 256]

  # === ACTION SMOOTHING VARIATIONS ===
  no_smoothing:
    name: "no_smoothing"
    description: "No action smoothing - raw muscle control"
    learning_rate: 2e-4
    clip_range: 0.15
    ent_coef: 0.005
    n_epochs: 8
    batch_size: 64
    reward_weights:
      tracking: 10.0
      height: 2.0
      velocity: 3.0
    action_smoothing: 0.0

  minimal_smoothing:
    name: "minimal_smoothing"
    description: "Minimal action smoothing"
    learning_rate: 2e-4
    clip_range: 0.15
    ent_coef: 0.005
    n_epochs: 8
    batch_size: 64
    reward_weights:
      tracking: 10.0
      height: 2.0
      velocity: 3.0
    action_smoothing: 0.03

  moderate_smoothing:
    name: "moderate_smoothing"
    description: "Moderate action smoothing for realism"
    learning_rate: 2e-4
    clip_range: 0.15
    ent_coef: 0.005
    n_epochs: 8
    batch_size: 64
    reward_weights:
      tracking: 10.0
      height: 2.0
      velocity: 3.0
    action_smoothing: 0.1

  heavy_smoothing:
    name: "heavy_smoothing"
    description: "Heavy action smoothing for biological realism"
    learning_rate: 2e-4
    clip_range: 0.15
    ent_coef: 0.005
    n_epochs: 8
    batch_size: 64
    reward_weights:
      tracking: 10.0
      height: 2.0
      velocity: 3.0
    action_smoothing: 0.2

  # === REWARD BALANCING EXPERIMENTS ===
  tracking_focused:
    name: "tracking_focused"
    description: "Heavy focus on expert trajectory tracking"
    learning_rate: 1.5e-4
    clip_range: 0.12
    ent_coef: 0.003
    n_epochs: 6
    batch_size: 96
    reward_weights:
      tracking: 20.0
      height: 1.0
      velocity: 1.0
    action_smoothing: 0.08

  velocity_focused:
    name: "velocity_focused"
    description: "Heavy focus on forward velocity"
    learning_rate: 3e-4
    clip_range: 0.18
    ent_coef: 0.008
    n_epochs: 8
    batch_size: 64
    reward_weights:
      tracking: 5.0
      height: 2.0
      velocity: 8.0
    action_smoothing: 0.12

  stability_focused:
    name: "stability_focused"
    description: "Focus on stability and height maintenance"
    learning_rate: 1e-4
    clip_range: 0.1
    ent_coef: 0.002
    n_epochs: 5
    batch_size: 128
    reward_weights:
      tracking: 8.0
      height: 5.0
      velocity: 2.0
    action_smoothing: 0.08

  balanced_rewards:
    name: "balanced_rewards"
    description: "Balanced reward weights"
    learning_rate: 2e-4
    clip_range: 0.15
    ent_coef: 0.005
    n_epochs: 8
    batch_size: 64
    reward_weights:
      tracking: 10.0
      height: 3.0
      velocity: 3.0
    action_smoothing: 0.1

  # === BATCH SIZE & EPOCH COMBINATIONS ===
  micro_batch:
    name: "micro_batch"
    description: "Very small batches with many epochs"
    learning_rate: 3e-4
    clip_range: 0.2
    ent_coef: 0.008
    n_epochs: 15
    batch_size: 16
    reward_weights:
      tracking: 10.0
      height: 2.0
      velocity: 3.0
    action_smoothing: 0.1

  small_batch:
    name: "small_batch"
    description: "Small batches for frequent updates"
    learning_rate: 2.5e-4
    clip_range: 0.18
    ent_coef: 0.007
    n_epochs: 12
    batch_size: 32
    reward_weights:
      tracking: 10.0
      height: 2.0
      velocity: 3.0
    action_smoothing: 0.1

  large_batch:
    name: "large_batch"
    description: "Large batches for stable gradients"
    learning_rate: 1.5e-4
    clip_range: 0.12
    ent_coef: 0.003
    n_epochs: 4
    batch_size: 256
    reward_weights:
      tracking: 10.0
      height: 2.0
      velocity: 3.0
    action_smoothing: 0.1

  mega_batch:
    name: "mega_batch"
    description: "Very large batches with few epochs"
    learning_rate: 1e-4
    clip_range: 0.1
    ent_coef: 0.002
    n_epochs: 2
    batch_size: 512
    reward_weights:
      tracking: 10.0
      height: 2.0
      velocity: 3.0
    action_smoothing: 0.1

  # === CLIPPING VARIATIONS ===
  no_clipping:
    name: "no_clipping"
    description: "No PPO clipping - trust region only via KL"
    learning_rate: 1e-4
    clip_range: 1.0
    ent_coef: 0.003
    n_epochs: 5
    batch_size: 128
    reward_weights:
      tracking: 10.0
      height: 2.0
      velocity: 3.0
    action_smoothing: 0.1

  tight_clipping:
    name: "tight_clipping"
    description: "Very tight clipping for conservative updates"
    learning_rate: 2e-4
    clip_range: 0.05
    ent_coef: 0.005
    n_epochs: 8
    batch_size: 64
    reward_weights:
      tracking: 10.0
      height: 2.0
      velocity: 3.0
    action_smoothing: 0.1

  loose_clipping:
    name: "loose_clipping"
    description: "Loose clipping for large policy changes"
    learning_rate: 2e-4
    clip_range: 0.3
    ent_coef: 0.005
    n_epochs: 8
    batch_size: 64
    reward_weights:
      tracking: 10.0
      height: 2.0
      velocity: 3.0
    action_smoothing: 0.1

  # === SPECIALIZED APPROACHES ===
  muscle_coordination:
    name: "muscle_coordination"
    description: "Optimized for muscle coordination learning"
    learning_rate: 1.5e-4
    clip_range: 0.12
    ent_coef: 0.004
    n_epochs: 6
    batch_size: 96
    reward_weights:
      tracking: 12.0
      height: 2.0
      velocity: 2.5
    action_smoothing: 0.15
    network_size: [512, 512, 256, 256]

  rapid_prototyping:
    name: "rapid_prototyping"
    description: "Fast learning for quick iteration"
    learning_rate: 8e-4
    clip_range: 0.25
    ent_coef: 0.015
    n_epochs: 15
    batch_size: 32
    reward_weights:
      tracking: 6.0
      height: 3.0
      velocity: 4.0
    action_smoothing: 0.18

  biological_realism:
    name: "biological_realism"
    description: "Maximum biological realism in muscle control"
    learning_rate: 8e-5
    clip_range: 0.08
    ent_coef: 0.001
    n_epochs: 4
    batch_size: 192
    reward_weights:
      tracking: 15.0
      height: 1.5
      velocity: 2.0
    action_smoothing: 0.25

  exploration_heavy:
    name: "exploration_heavy"
    description: "Maximum exploration for 80D muscle space"
    learning_rate: 4e-4
    clip_range: 0.25
    ent_coef: 0.03
    n_epochs: 12
    batch_size: 48
    reward_weights:
      tracking: 5.0
      height: 3.0
      velocity: 5.0
    action_smoothing: 0.2

  fine_tuning:
    name: "fine_tuning"
    description: "Fine-tuning approach with minimal changes"
    learning_rate: 3e-5
    clip_range: 0.05
    ent_coef: 0.0005
    n_epochs: 3
    batch_size: 256
    reward_weights:
      tracking: 18.0
      height: 1.0
      velocity: 1.5
    action_smoothing: 0.05

  # === MIXED STRATEGIES ===
  progressive_learning:
    name: "progressive_learning"
    description: "Start conservative, increase exploration over time"
    learning_rate: 1.5e-4
    clip_range: 0.1
    ent_coef: 0.002
    n_epochs: 6
    batch_size: 96
    reward_weights:
      tracking: 12.0
      height: 2.0
      velocity: 2.5
    action_smoothing: 0.1

  adaptive_approach:
    name: "adaptive_approach"  
    description: "Adaptive parameters for different learning phases"
    learning_rate: 2e-4
    clip_range: 0.15
    ent_coef: 0.006
    n_epochs: 8
    batch_size: 64
    reward_weights:
      tracking: 9.0
      height: 2.5
      velocity: 3.0
    action_smoothing: 0.12

  robustness_test:
    name: "robustness_test"
    description: "Test robustness with varied parameters"
    learning_rate: 2.5e-4
    clip_range: 0.16
    ent_coef: 0.007
    n_epochs: 9
    batch_size: 72
    reward_weights:
      tracking: 8.5
      height: 2.2
      velocity: 3.3
    action_smoothing: 0.11

# Default settings for all experiments
defaults:
  total_timesteps: 1000000
  n_envs: 6
  gamma: 0.99
  gae_lambda: 0.95
  vf_coef: 0.5
  max_grad_norm: 0.5
  network_size: [512, 512, 256, 256]
  device: "cpu"
  plot_interval: 20000
  eval_freq: 50000
  save_freq: 100000 